
@article{chipman_bart_2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	issn = {1932-6157},
	shorttitle = {{BART}},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-1/BART-Bayesian-additive-regression-trees/10.1214/09-AOAS285.full},
	doi = {10.1214/09-AOAS285},
	language = {en},
	number = {1},
	urldate = {2023-02-08},
	journal = {Ann. Appl. Stat.},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
	keywords = {BART},
}

@article{linero_bayesian_2018,
	title = {Bayesian {Regression} {Trees} for {High}-{Dimensional} {Prediction} and {Variable} {Selection}},
	volume = {113},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1264957},
	doi = {10.1080/01621459.2016.1264957},
	abstract = {Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonparametric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of observations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets. Supplementary materials for this article are available online.},
	language = {en},
	number = {522},
	urldate = {2023-02-10},
	journal = {Journal of the American Statistical Association},
	author = {Linero, Antonio R.},
	month = apr,
	year = {2018},
	pages = {626--636},
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: {A} gradient boosting machine.},
	volume = {29},
	issn = {0090-5364},
	shorttitle = {Greedy function approximation},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
	doi = {10.1214/aos/1013203451},
	language = {en},
	number = {5},
	urldate = {2023-07-05},
	journal = {Ann. Statist.},
	author = {Friedman, Jerome H.},
	month = oct,
	year = {2001},
}

@article{apley_visualizing_2020,
	title = {Visualizing the {Effects} of {Predictor} {Variables} in {Black} {Box} {Supervised} {Learning} {Models}},
	volume = {82},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/82/4/1059/7056085},
	doi = {10.1111/rssb.12377},
	abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
	language = {en},
	number = {4},
	urldate = {2023-11-30},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Apley, Daniel W. and Zhu, Jingyu},
	month = sep,
	year = {2020},
	pages = {1059--1086},
}
